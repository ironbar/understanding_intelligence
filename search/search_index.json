{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Understanding Intelligence","text":"<p>In this repo I will store the knowledge of the quest of understanding intelligence. The structure of the wiki is very likely to change through time.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are always welcome!</p> <p>You can contribute through github or you can contact me using any of the social media that you will find at the bottom of the page.</p>"},{"location":"artificial-intelligence/01-dalle2/","title":"Dall-e 2","text":"<p>Dall-e 2 is trained with a dataset of 650M of image, text pairs. It shows an incredible ability to generalize, creating realistic images for very unusual text prompts.</p> <p>Why are the key ingredients for generalization on Dall-e 2?</p> <ul> <li>Huge dataset of text, image pairs</li> <li>Big model (although is not huge because at 3B parameters is much smaller than the 175B parameters of GPT3)</li> </ul> <p>Compared to models that are trained on Imagenet with a fixed set of categories the flexibility of a model trained with natural language is vastly superior.</p> <p>I guess it will be possible to do the same with different pairs:</p> <ul> <li>text, text -&gt; GPT3</li> <li>text and video</li> <li>text and music</li> <li>text and actions</li> </ul> <p>So language seems to be a key ingredient for achieving generalization.</p>"},{"location":"artificial-intelligence/01-dalle2/#how-it-works-in-a-nutshell","title":"How it works in a nutshell","text":"<ol> <li>CLIP model is trained to create embeddings from image, text pairs that are as similar as possible</li> <li>Dall-e 2 first stage is a model that takes a CLIP text embedding as input and creates an image embedding</li> <li>Dall-e 2 second stage decodes the image embedding and creates a new image using a diffusion model</li> </ol>"},{"location":"artificial-intelligence/01-dalle2/#the-role-of-embeddings","title":"The role of embeddings","text":"<p>It seems that an embedding is a universal representation method. There are word embeddings, text embeddings, biometric embeddings, image embeddings...</p> <p>From the point of view of information an embedding is a sequence of numbers. I could encode a maximum of different <code>m^n</code> states in an embedding of length <code>n</code> if each dimension has <code>m</code> bits. The biggest clip embedding size is 1024. In this context the number of bits will be how many different partitions we can make of each dimension. With float32 each dimension has 4 bytes so that would be a total of 32 bits that will theoretically enable a partition of 2^32. However neural networks are noisy so let's take a pessimistic lower bound that says that each dimension can only hold two partitions (positive and negative). In that case the number of different states that the embedding could store is <code>2^1024 ~ 10^313</code>, a number much bigger than <code>10^82</code> which is the number of atoms in the observable universe.</p> <p>Thus we can see that the embeddings can encode a ton of information. However to be able to use that information we need a big encoder or decoder, because the embedding alone does not mean anything.</p>"},{"location":"artificial-intelligence/02-gato/","title":"Gato","text":"<p>Gato, a generalist agent, is a paper released by Deepmind short after Flamingo on May 2022.</p> <p></p> <p>The idea is to train a model that can receive both text and images as inputs (like Flamingo) in a bunch of different tasks. Gato is much smaller than Flamingo: 1.2B vs 80B. This is motivated because they want to be able to execute the model in real time for robotics tasks.</p> <p>The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.</p> <p>The guiding design principle of Gato is to train on the widest variety of relevant data possible, including diverse modalities such as images, text, proprioception, joint torques, button presses, and other discrete and continuous observations and actions. To enable processing this multi-modal data, we serialize all data into a flat sequence of tokens. In this representation, Gato can be trained and sampled from akin to a standard large-scale language model.</p> <p>While no agent can be expected to excel in all imaginable control tasks, especially those far outside of its training distribution, we here test the hypothesis that training an agent which is generally capable on a large number of tasks is possible; and that this general agent can be adapted with little extra data to succeed at an even larger number of tasks.</p> <p>We focus our training at the operating point of model scale that allows real-time control of real-world robots, currently around 1.2B parameters in the case of Gato. As hardware and model architectures improve, this operating point will naturally increase the feasible model size, pushing generalist models higher up the scaling law curve. For simplicity Gato was trained offline in a purely supervised manner; however, in principle, there is no reason it could not also be trained with either offline or online reinforcement learning (RL).</p> <p>That is true but RL is much harder than supervised learning.</p> <p>As described above, Gato\u2019s network architecture has two main components: the parameterized embedding function which transforms tokens to token embeddings, and the sequence model which outputs a distribution over the next discrete token. While any general sequence model can work for next token prediction, we chose a transformer (Vaswani et al., 2017) for simplicity and scalability. Gato uses a 1.2B parameter decoder-only transformer with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196 (more details in Section C.1).</p> <p></p> <p>As shown in the picture above Gato benefits from the same scaling rules that apply to the transformers.</p> <p>Transformer sequence models are effective as multi-task multi-embodiment policies, including for real-world text, vision and robotics tasks. They show promise as well in few-shot out-of-distribution task learning. In the future, such models could be used as a default starting point via prompting or fine-tuning to learn new behaviors, rather than training from scratch.</p> <p>Gato is not a zero-shot model like Dall-e or Flamingo, the results are not as impressing. It needs fine-tuning to be able to achieve good scores on new games.</p> <p>Given scaling law trends, the performance across all tasks including dialogue will increase with scale in parameters, data and compute. Better hardware and network architectures will allow training bigger models while maintaining real-time robot control capability. By scaling up and iterating on this same basic approach, we can build a useful general-purpose agent.</p> <p>In essence Gato is a promise of something bigger that will come in the future. It is possible that scaling Gato may result in a general zero-shot policy that might be able to do many different tasks.</p>"},{"location":"artificial-intelligence/02-gato/#criticism","title":"Criticism","text":"<p>Gato is a glimpse of what could come in the future, but we already know that it is limited. A policy is very unlikely to achieve human intelligence because humans have the ability to plan, while a policy is similar to a fast reaction.</p> <p>In the book \"Think fast, think slow\" it was talked about two systems:</p> <p>System 1 thinking is a near-instantaneous process; it happens automatically, intuitively, and with little effort. It's driven by instinct and our experiences. System 2 thinking is slower and requires more effort. It is conscious and logical.</p> <p>I believe Gato is a very good candidate to become a powerful System 1, but System 2 is needed to do more complex tasks. For example when playing Go Deepmind needed to use Monte Carlo for sampling although they had a very good policy. However the policy was never as strong as planning.</p>"},{"location":"artificial-intelligence/03-flamingo/","title":"Flamingo","text":"<p>Flamingo is a visual language model developed by DeepMind on April 2022. After the release of the paper and blogpost some impressive dialogs have been shown on twitter.</p> <p></p> <p></p> <p></p> <p>Flamingo is able to receive text, images and video interleaved and returns text as output.</p>"},{"location":"artificial-intelligence/03-flamingo/#how-it-works","title":"How it works?","text":"<p>As a start point they train a visual only and text only models. The visual model is trained like OpenAI's CLIP. Those models are then frozen and introduced as modules in a new and bigger model that learns to combine the text and visual information. This new model is trained on multimodal datasets gathered from internet.</p> <p></p>"},{"location":"artificial-intelligence/03-flamingo/#thoughts","title":"Thoughts","text":"<p>It is likely that the same architecture could be used to integrate audio as input and maybe other kind of inputs.</p> <p>The published results show signs of reasoning and understanding of the inputs. I find it very surprising the abilities to count objects and to describe the position of the objects. As far as I remember Dall-e 2 struggle with this abilities because CLIP objective does not capture that information.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/","title":"Tesla AI Day 2021","text":"<p>I have been watching the presentation of the Tesla AI Day and I would like to summarize how the system works.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#problem","title":"Problem","text":"<p>The problem the Tesla car has to solve is to go from point A to point B. It needs to navigate through roads and cities to arrive to the goal location. It needs to deal with traffic, pedestrians and obstacles.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#route-planning","title":"Route planning","text":"<p>The first step of the process is to do route planning. Apparently Google Maps uses Dijkstra's algorithm to find the shortest path on a graph.</p> <p>So the car knows its location because it has a GPS and the user enters the goal location on the touchscreen. A search algorithm returns a route that may be decomposed into a series of steps. My intuition is that this steps are fed as high level actions to the AI system that translates those high level actions into fine-grained actions such as steer the wheels, brake...</p> <p>For example the high level actions could be: turn left, follow the road for 20 km... The typical instructions we hear when using the GPS.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#perception","title":"Perception","text":"<p>The task of the perception system is to integrate over time the information of all the cameras and sensors and to create an internal representation of the state of the world. This representation will be used later for planning. The representation is also rendered in the touchscreen probably to give confidence to the user.</p> <p></p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#integrating-all-the-cameras","title":"Integrating all the cameras","text":"<p>Each camera is processed with a visual backbone and a Transformer learns to combine the information of the cameras into a single vector representation. Thus the output of this module is a single embedding that has all the information provided by all the cameras. From the presentation it seems that this representation has the same shape as the final egocentric representation of the world that will be described on the section below.</p> <p></p> <p>I find very interesting the use of a Transformer to merge information from different sources. It feels a very good choice and probably could be applied to many different problems.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#integrating-information-over-time","title":"Integrating information over time","text":"<p>To be able to represent the environment the car needs to integrate information over time. Objects can be occluded, signs could be left behind... and the car needs to remember that information.</p> <p></p> <p>This is solved with a queue that stores the vector space embeddings. That queue is updated using both time and space criteria.</p> <p></p> <p>Finally there is a there is a grid of RNN that creates and egocentric representation of the world. This RNN module integrates the time information into a single and final representation of the world.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#whole-architecture","title":"Whole architecture","text":"<p>It is curious that the representation of the world can be directly visualized. The picture above shows the activation of some of the channels of the world representation.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#planning","title":"Planning","text":"<p>They apply planning in the vector space to optimize for safety, comfort and efficiency.</p> <p></p> <p>According to the presentation the most efficient search method is to use Muzero's Monte Carlo Tree Search to plan.</p> <p></p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#training-data","title":"Training data","text":"<p>The models are trained to predict the vector space given the images of the individual cameras.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#real-data","title":"Real data","text":"<p>The labels are applied in the vector space and reprojected to the individual cameras. Maybe they use this as auxiliary output or for visualization purposes, more probably the first option.</p> <p></p> <p>They have 1000 in house labellers and they have developed all the labelling software. Also auto-labelling is done offline to improve the speed of the labellers.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#simulation","title":"Simulation","text":"<p>In addition to real data they also have a simulation that provides data with perfect labels and also allows to reproduce clips from the real world with variations.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#summary","title":"Summary","text":"<ol> <li>Route planning</li> <li>Perception to create a world representation</li> <li>Plan the car trajectory using the world representation</li> </ol> <p>Overall it seems a pretty strong setup and Tesla seems to be very well positioned to achieve autonomous driving. The model architecture is sound, the data collection pipeline is effective...</p> <p>Can I think of any obstacle that given the current information may block the progress of Tesla to autonomous driving?</p> <ul> <li>Some people talk about long tail distributions. Rare events that are difficult to collect. This would be a problem to all automakers. And Tesla has the biggest fleet so far so it's in the best position.</li> <li>A more powerful computer may be needed to deploy autonomous driving. This should not be a problem for Tesla, it that is the case it could sell it to its costumers as an upgrade.</li> </ul> <p>At this moment I cannot imagine an obstacle, so apparently is just a matter of time that Tesla gets to autonomous driving. They seem to have all the needed ingredients.</p>"},{"location":"artificial-intelligence/04-tesla-ai-day-2021/#thoughts","title":"Thoughts","text":"<p>It's an hybrid system because it combines different kinds of algorithms.</p> <p>In this context the state of the world is the location, speed, acceleration and state of all the elements of the road.</p> <p>The finality of the sensors/senses is to find the state of the world. That is true for every system and living being. So it is nice that in the Tesla car we find that goal very explicitly.</p> <p>What is a good representation of the world? In this case it is forced to be the elements that are used later for planning. Another view would be all the necessary information to predict the future (world model). A more narrow view would be all the information needed to maximize the reward (value function or policy). The first one is more general but needs more capacity. The other one is simpler but will be more difficult to adapt to changes in the environment.</p> <p>Humans create an internal representation of the world and they live in that representation. We constantly create predictions about the future and update our beliefs if they fail. Those predictions can have different timescales.</p> <p></p> <p>The world model can be queried to:</p> <ul> <li>Recall elements from the past</li> <li>Imagine or predict future situations</li> </ul> <p>It feels like a module is missing on the previous drawing. Some sequential module that we could call consciousness and whose task is to direct the flow of information. It can use the world model for planning, understand if the goal is met, use the system 1 when possible... A module like that could be trained with reinforcement learning.</p> <p></p> <p>I should think more deeply about the world model and the director modules on coming weeks.</p>"},{"location":"artificial-intelligence/05-active-dendrites/","title":"Avoiding Catastrophical Forgetting with Active Dendrites","text":"<p>Paper and Yannic Kilcher video</p>"},{"location":"artificial-intelligence/05-active-dendrites/#summary","title":"Summary","text":"<p>Standard Artificial Neural Networks (ANNs) often fail dramatically when learning multiple tasks, a phenomenon known as catastrophic forgetting where the network forgets previously learned information.</p> <p>Today it is well known that the point neuron assumption is an oversimplified model of biological computations. Proximal synapses (close to the cell body) have a linear impact on the neuron, but the vast majority of synapses are located on distal dendritic segments (far from the cell body) and individually have minimal impact on the cell. These distal segments process groups of synapses locally in a non-linear fashion, and are known as active dendrites . Empirical evidence suggests that each distal dendritic segment acts as a separate active subunit performing its own local computation. Modeling studies show that neurons with active dendrites are more powerful and complex than the point neuron model can accommodate.</p> <p>This suggests that basal active dendrites have a modulatory, long-lasting impact on the cell\u2019s response, with a very different role than proximal, or feedforward, inputs</p> <p>Neural circuits in the neocortex are highly sparse.</p> <p></p> <p>The idea is to use a context to modulate the activation of the neurons and next to induce sparsity with a k winners take it all layer (kWTA). This two modifications to the standard neural network work together to do modify only a small sparse subset of the network for each input.</p> <p>This happens because the kWTA blocks the gradients of all the units that are set to zero. And the context modulates the activations, thus influencing which units will be zeroed and which won't.</p> <p>So in essence is like having multiple subnetworks inside a single network.</p>"},{"location":"artificial-intelligence/05-active-dendrites/#thoughts","title":"Thoughts","text":"<p>The proposed architecture is interesting and could be an inspiration for future custom networks. I would like to see if this could scale to bigger models.</p> <p>I have found one sentence of the paper very intriguing:</p> <p>. Liu et al. [2019], Maninis et al. [2019] demonstrate that attention-based architectures could also prevent task interference in multi-task learning scenarios</p> <p>Is this implying that transformers do not suffer from catastrophical forgetting in a multi-task learning scenario?</p> <p>The cited papers are End-to-End Multi-Task Learning with Attention and Attentive Single-Tasking of Multiple Tasks. Those papers do not talk about transformers but about adding an attention mechanism can help to multi-task learning. Since the transformer architecture is based on attention we could make the assumption that it will also have good properties for multi-task learning. And moreover we have the recent publication of Gato that shows that a transformer can learn to do a lot of tasks.</p> <p>So probably choosing a transformer as the architecture has more sense than implementing the proposed changes on a more classic architecture.</p>"},{"location":"artificial-intelligence/06-model-based-rl/","title":"Model Based Reinforcement Learning","text":""},{"location":"artificial-intelligence/06-model-based-rl/#motivation","title":"Motivation","text":"<p>In the past I have followed DeepMind's RL course and now I'm doing the Hugginface's RL course to remember the concepts. However they do not dig deep into model based RL and that's what I want to do it now.</p> <p>In Sutton and Barto book I cannot find either anything about model based RL.</p> <p>I believe world models will play a big role in the road of AGI. Without a world model is not possible to plan, so we need them to achieve complex thinking and reasoning.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#resources","title":"Resources","text":"<p>Let's search for learning resources and list them to prioritize them.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#youtube","title":"Youtube","text":"<ul> <li>Model-free vs Model-based Reinforcement Learning -- Oriol Vinyals (11/10/2020) A talk about AlphaGo, AlphaStar, Alphazero... Some initial ideas about the differences between model-free and model-based but most of the talk is about the experience of Oriol with those mentioned projects.</li> <li>World-Models \ud83c\udf0d Model Based Reinforcement Learning. Describes two challenges of world models: partial observability and randomness</li> <li>Pieter Abbel's Lecture on Model-based RL In this video the model is used to improve the policy. The policy is trained using the world model skipping the simulator. That results in better sample efficiency. However it is very different of the planning approach I'm interested in.</li> <li>Yannic Kilcher World Models</li> <li>Yannic Kilcher Learning model-based planning from scratch Describes a model   that learns to plan. Instead of going to an algorithm like MCTS the model learns how to prioritize   the planning steps (budget, depth and breath)</li> <li>Dreamer v2: Mastering Atari with Discrete World Models</li> <li>Harri Valpola: System 2 AI and Planning in Model-Based Reinforcement Learning</li> </ul>"},{"location":"artificial-intelligence/06-model-based-rl/#papers","title":"Papers","text":"<ul> <li>Model-based Reinforcement Learning: A Survey This is a very good   review of all the options when taking the model-based rl approach.</li> </ul>"},{"location":"artificial-intelligence/06-model-based-rl/#world-models","title":"World models","text":"<p>The number of parameters of C, a linear model, is minimal in comparison. This choice allows us to explore more unconventional ways to train C\u2009\u2014\u2009for example, even using evolution strategies to tackle more challenging RL tasks where the credit assignment problem is difficult.</p> <p>I find this cite very interesting. So apparently there are challenging tasks were typical RL strategies fails and evolutionary approaches are needed.</p> <p>To summarize the Car Racing experiment, below are the steps taken:</p> <ol> <li>Collect 10,000 rollouts from a random policy.</li> <li>Train VAE (V) to encode each frame into a latent vector z</li> <li>Train MDN-RNN (M) to model <code>P(z_t+1 | a_t, z_t, h_t)</code></li> <li>Evolve Controller (C) to maximize the expected cumulative reward of a rollout.</li> </ol> <p>It is possible to train the model inside the \"dream\" and transfer the policy to the environment. To be able to that it's necessary to carefully control the temperature when sampling to avoid overfitting to the world model.</p> <p>In our experiments, the tasks are relatively simple, so a reasonable world model can be trained using a dataset collected from a random policy. But what if our environments become more sophisticated? In any difficult environment, only parts of the world are made available to the agent only after it learns how to strategically navigate through its world. For more complicated tasks, an iterative training procedure is required. We need our agent to be able to explore its world, and constantly collect new observations so that its world model can be improved and refined over time.</p> <p>An iterative process is needed for the real world.</p> <p>The choice of implementing V as a VAE and training it as a standalone model also has its limitations, since it may encode parts of the observations that are not relevant to a task. After all, unsupervised learning cannot, by definition, know what will be useful for the task at hand. For instance, our VAE reproduced unimportant detailed brick tile patterns on the side walls in the Doom environment, but failed to reproduce task-relevant tiles on the road in the Car Racing environment. By training together with an M that predicts rewards, the VAE may learn to focus on task-relevant areas of the image, but the tradeoff here is that we may not be able to reuse the VAE effectively for new tasks without retraining.</p> <p>That point of view is very interesting. I haven't imagined that using reward as target could harm the generalization of the policy.</p> <p>Summary: In this paper the world model is not used for planning but for providing a good representation of the world state that a very simple policy can use to learn. They force the policy to be small because they use an evolutionary method to improve the policy.</p> <p>As a final note from the paper Qualitative Differences Between Evolutionary Strategies and Reinforcement Learning Methods for Control of Autonomous Agents</p> <p>EAs do not suffer from the sparsity of the reward since they operate on the basis of a fitness measure that encodes the sum of the rewards collected during evaluation episodes. RLs instead, which operate by associating rewards to specific actions, struggle with temporal credit assignment when rewards are sparse. Temporal difference in RL use bootstrapping to better handle this aspect but still struggles with sparse rewards when the time horizon is long.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#dreamer","title":"Dreamer","text":"<ul> <li>Dream to Control: Learning Behaviors by Latent Imagination</li> <li>Mastering Atari with Discrete World Models</li> </ul> <p>This is a continuation of the World Models paper. It takes the idea of training on the dream world model to the extreme.</p> <p>We learn the world model from a dataset of past experience, learn an actor and critic from imagined sequences of compact model states, and execute the actor in the environment to grow the experience dataset.</p> <p>A virtuous dynamics emerges from this approach:</p> <ol> <li>If the world model is correct then the agent will learn a good policy for the real world</li> <li>If the world model is incorrect the agent will learn a bad policy, but when playing on the real world with that policy it will gather new data that will help to correct the world model</li> </ol> <p>Thus given enough time the algorithm will converge: the world model will be good enough and the agent will learn a good policy that will transfer to the real world. Humans do also correct their world model when they find unexpected situations.</p> <p>The advantages of this approach are:</p> <ul> <li>Faster simulation. The world model uses the latent space for predicting the transitions and that results on fast predictions. Moreover the GPU can be used to run the model much faster than the CPU</li> <li>Better data efficiency. Model based RL methods are more data efficient than model free.</li> </ul> <p></p> <p>It is similar to a Vector Quantized autoencoder but not exactly the same.</p> <p>The world model is not used to plan, but to train a policy on the world model. This allows to use 10000 times more data than the real interactions with the environment.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#planet-a-deep-planning-network-for-reinforcement-learning","title":"PlaNet: A Deep Planning Network for Reinforcement Learning","text":"<p>In this case the agent needs a world model that it is used to plan the next action. It does not learn a value or policy function like the previous examples. The planning method does not seem to be very advanced or intelligent: as far as I understand it tries random sequences of actions in a monte carlo setup and it takes the first action of the best sequence.</p> <p></p> <p>PlaNet solves a variety of image-based control tasks, competing with advanced model-free agents in terms of final performance while being 5000% more data efficient on average.</p> <p>This is another evidence that model-based methods are more data efficient.</p> <p>In short, PlaNet learns a dynamics model given image inputs and efficiently plans with it to gather new experience. In contrast to previous methods that plan over images, we rely on a compact sequence of hidden or latent states. This is called a latent dynamics model: instead of directly predicting from one image to the next image, we predict the latent state forward.</p> <p>Compared to our preceding work on world models, PlaNet works without a policy network -- it chooses actions purely by planning, so it benefits from model improvements on the spot.</p> <p>I find Planet very interesting, but it is very likely that it could improve planning by adding a policy and a value function. I believe that is what we are going to find in the next MuZero paper.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#muzero-mastering-go-chess-shogi-and-atari-without-rules","title":"MuZero: Mastering Go, chess, shogi and Atari without rules","text":"<p>When planning Muzero expands first the actions with the higher value and the higher probability. Eventually all options are explored if the number of simulations is big enough.</p> <p></p> <p>The model is trained to learn the following functions: state encoding, transition function, policy and value. As output receives the rewards and the results of the searches. Thus the state encoding and transition function are learned indirectly, they need to work well in order to be able to predict the policy and value.</p> <p>In this paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for future work.</p> <p>This is the main limitation of Muzero, does not work for stochastic environments. To be able to cope with the partial observability of Atari 32 frames are fed to the model to encode the state.</p> <p>For each board game 1000 TPUs were used for training, so that is a lot of compute power.</p> <p>So this approach like PlaNet learns a model of the world and it uses for planning. However the planning strategy used by MuZero is more advanced than the used at PlaNet.</p> <p>It's interesting to see how the ability of the model increases with the size of the search. A difference of 1400 in elo score is massive, it means that it is almost impossible to win.</p> <p></p>"},{"location":"artificial-intelligence/06-model-based-rl/#mastering-atari-games-with-limited-data","title":"Mastering Atari Games with Limited Data","text":"<p>In this paper they make 3 modifications to Muzero to be able to train on a small number of frames (100k)</p> <p>In previous MCTS RL algorithms, the environment model is either given or only trained with rewards, values, and similarity policies, which cannot provide sufficient training signals due to their scalar nature.</p> <p>To solve the problem they use the observations of the next state also as a target. In the Muzero video this was mentioned and it did not brought improvements. So it seems that it helps in the small data regime.</p> <p>If we only see the first observation, along with future actions, it is very hard both for an agent and a human to predict at which exact future timestep the player would lose a point. However, it is easy to predict the agent will miss the ball after a game. In this case, the right player didn\u2019t move sufficient number of timesteps if he does not and missed the ball.</p> <p>The point here is to predict a single value prefix instead of multiple intermediate rewards.</p> <p>This value target suffers from off-policy issues, since the trajectory is rolled out using an older policy, and thus the value target is no longer accurate. When data is limited, we have to reuse the data sampled from a much older policy, thus exaggerating the inaccurate value target issue we propose to use rewards of a dynamic horizon l from the old trajectory, where l &lt; k and l should be smaller if the trajectory is older</p> <p>By doing this 3 changes they are able to have good results when training just on 100k frames, for example DQN uses 200 million frames.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#vector-quantized-models-for-planning","title":"Vector Quantized Models for Planning","text":"<p>This paper extends the Muzero approach to stochastic and partially observable environments. To be able to do that it learns a model of the environment that includes the opponent player if there is any. For example on Muzero when playing chess the agent first chooses a move and then chooses a move for the opponent for planning. Instead this agent chooses a move for itself and then samples a transition from the environment.</p> <p>One drawback of this paper is that it seems to be supervised training, not self-play.</p> <p>The state is quantized so it is easier to sample from it.</p> <p>It is able to work both on Chess and DeepMind Lab.</p>"},{"location":"artificial-intelligence/06-model-based-rl/#summary","title":"Summary","text":"<p>We have seen many ways of learning and using world models. Probably the closer one to how the brain works is the Vector Quantized Models for Planning because it learns a model of the environment and uses it for planning.</p> <p>One good thing of this kind of approach is that the model has a policy that could be used to take fast decisions (System 1) but can also use the world model for planning (System 2)</p> <p>The brain world model is more powerful because it allows to plan in different time scales and it also can be queried using human language.</p>"},{"location":"artificial-intelligence/07-director-hierarchical-planning/","title":"Director - Deep Hierarchical Planning from Pixels","text":""},{"location":"artificial-intelligence/07-director-hierarchical-planning/#summary-of-the-paper","title":"Summary of the paper","text":"<p>current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets</p> <p>This is true, current RL has difficulties on environments with very sparse rewards.</p> <p>Hierarchical reinforcement learning (HRL) aims to automatically break long-horizon tasks into subgoals or commands that are easier to achieve, typically by learning high-level controllers that operate at more abstract time scales and provide commands to low-level controllers that select primitive actions.</p> <p>In this paper, we present Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. We observe the effectiveness of Director on long-horizon tasks with very sparse rewards and demonstrate its generality by learning successfully in a wide range of domains.</p> <p>The world model representations st are 1024-dimensional continuous vectors. Selecting such representations as goals would be challenging for the manager because this constitutes a very high- dimensional continuous action space. To avoid a high-dimensional continuous control problem for the manager, Director compresses the representations st into smaller discrete codes z using a goal autoencoder that is trained on replay buffer model states</p> <p>This is interesting, they use a quantized autoencoder to compress the state and to plan in a smaller space. Sampling in a discrete space is easier.</p> <p>Director learns a manager policy that selects a new goal for the worker every fixed number of K =time steps. The manager is free to choose goals that are much further than 8 steps away from the current state, and in practice, it often learns to choose the most distant goals that the worker is able to achieve. Instead of selecting goals in the high-dimensional continuous latent space of the world model, the manager outputs abstract actions in the discrete code space of the goal autoencoder (Section 2.2). The manager actions are then decoded into world model representations before they are passed on to the worker as goals. To select actions in the code space, the manager outputs a vector of categorical distributions, analogous to the goal encoder in Section 2.2:</p> <p>The objective for the manager is to maximize the discounted sum of future task rewards and ex- ploration rewards.</p> <p>So the manager asks to the worker to achieve a future world state. It has to both maximimize the rewards and to explore the world.</p> <p>The manager takes decisions every 8 steps, so it is working on a different timescale than the worker.</p> <p>The worker is responsible for reaching the goals chosen by the manager. Because the manager outputs codes z in the discrete space of the goal autoencoder, we first decode the goals into the state space . of the world model g = dec(z). Conditioning the worker on decoded goals rather than the discrete codes has the benefit that its learning becomes approximately decoupled from the goal autoencoder. The worker policy is conditioned on the current state and goal, which changes every K = 8 time steps, and it produces primitive actions at to reach the feature space goal:</p> <p>The worker is a conditioned policy that tryies to achieve the goal state.</p>"},{"location":"artificial-intelligence/07-director-hierarchical-planning/#multi-level-hierarchical-planning","title":"Multi level hierarchical planning","text":"<p>On the paper there are only two levels for planning: high level and low level. Is it possible to have more levels? How would that work?</p> <p>Let's think of some examples of hierarchical planning:</p> <ol> <li>In a company there are different levels of abstractions. The bigger the company the longer the chain of command.    Each element of the chain is working on a different level of abstraction, being the higher the CEO of the    company and the lower a developer who is implementing a product. Each element has different goals    and rewards and works with different information. But in the end the whole company is working    to achieve the goals of the CEO (ideally).</li> <li>Every task can be decomposed on subtasks, and those subtasks can likely be decomposed on subtasks    and so on. We can have an arbitrary level of hierarchies when decomposing a task.</li> </ol> <p>Thus to have a multi-level hierarchical planning we need:</p> <ul> <li>Different managers operating at different time scales that decompose a high level goal into multiple   low level goals. Each manager tries to achieve the high level goal by proposing the low level goals.   Exploration would also be encouraged like in the director paper. The number of managers will   be a design choice.</li> <li>A low level worker that takes actions to try to achieve the received goals</li> <li>On each hierarchical level there might be different specialized managers or workers. For example   the agent might have to work on different contexts and having specialized policies could be better than a single policy</li> <li>All the hierarchical levels might operate on the same representation of the world, but it is likely   that using different levels of abstraction might be helpful (just like in the company example)</li> </ul>"},{"location":"artificial-intelligence/08-question-answering/","title":"Question Answering","text":""},{"location":"artificial-intelligence/08-question-answering/#intro","title":"Intro","text":"<p>On this page I want to study the state of the art of the question answering systems. I believe that a system that could answer correctly to questions about a document or a website will be very valuable. It would be a very useful tool for knowledge management, we could simply add knowledge to a website and then query the question answering model when we need it.</p> <p>On a small scale it would be useful for companies. It would \"replace\" that person in the company that knows everything.</p> <p>On a big scale it could replace Google and help advance scientific discovery.</p>"},{"location":"artificial-intelligence/08-question-answering/#open-questions","title":"Open questions","text":"<p>I have written a list of questions that I have at the start of this study that I would like to answer.</p>"},{"location":"artificial-intelligence/08-question-answering/#how-good-are-big-language-models-at-question-answering","title":"How good are big language models at question answering?","text":"<p>Remember that they typically train for one epoch so they don't memorize the dataset.</p>"},{"location":"artificial-intelligence/08-question-answering/#what-if-we-simply-fine-tune-a-big-model-in-our-data","title":"What if we simply fine-tune a big model in our data?","text":"<p>In the Stanford Lecture they do that with good results. However that might be too expensive to do for every user.</p>"},{"location":"artificial-intelligence/08-question-answering/#can-we-decouple-data-and-model","title":"Can we decouple data and model?","text":""},{"location":"artificial-intelligence/08-question-answering/#blenderbot","title":"Blenderbot","text":""},{"location":"artificial-intelligence/08-question-answering/#could-we-create-a-knowledge-graph-a-la-google-style","title":"Could we create a knowledge graph a la Google style?","text":""},{"location":"artificial-intelligence/08-question-answering/#how-do-humans-do-this-task-of-question-answering","title":"How do humans do this task of question answering?","text":""},{"location":"artificial-intelligence/08-question-answering/#is-the-system-scalable-to-challenge-google","title":"Is the system scalable? (to challenge Google)","text":""},{"location":"artificial-intelligence/08-question-answering/#are-there-commercial-or-open-source-solutions","title":"Are there commercial or open-source solutions?","text":"<p>No paid traffic on Google</p>"},{"location":"artificial-intelligence/08-question-answering/#resources","title":"Resources","text":""},{"location":"artificial-intelligence/08-question-answering/#huggingface-what-is-question-answering","title":"Huggingface: What is question answering?","text":""},{"location":"artificial-intelligence/08-question-answering/#stanford-cs224n-nlp-with-deep-learning-winter-2021-lecture-11-question-answering","title":"Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 11 - Question Answering","text":"<p>Video</p> <ul> <li>Pretrain is very important. It allows to improve F1 score on Squad V1 dataset from 77 to 95</li> <li>Question answering is not solved yet, the models fail against adversarial examples</li> <li>A lot of common knowledge is needed to answer some questions. For example knowing that a name is female, numeric knowledge...</li> <li>A big language model can directly answer questions after fine-tuning</li> <li>There is a very cool and very fast demo at the end about open-domain question answering</li> </ul>"},{"location":"artificial-intelligence/08-question-answering/#real-time-open-domain-qa-with-dense-sparse-phrase-index","title":"Real-Time Open-Domain QA with Dense-Sparse Phrase Index","text":"<p>Github | Paper | Live demo</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/","title":"Deep Learning evolution","text":""},{"location":"artificial-intelligence/09-deep-learning-evolution/#history-of-deep-learning","title":"History of Deep learning","text":"<p>\u25b6\ufe0f Deep Learning History and Recent Timeline</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#1998-lenet","title":"1998 Lenet","text":"<p>Back in 1998 Yann Lecun was using convolutional neural networks to recognize handwritten digits.</p> <p></p> <p>Neural networks were smalls and datasets were also small at that time.</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#2012-imagenet","title":"2012 Imagenet","text":"<p>On 2012 AlexNet won the Imagenet challenge. This is the start of the modern deep learning era where 3 factors were combined:</p> <ul> <li>Big datasets</li> <li>GPU compute power</li> <li>Algorithmic advances</li> </ul>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#2019-gpt-2","title":"2019 GPT-2","text":"<p>On 2017 the paper \"Attention is all you need\" was published. This paper introduced the Transformer architecture which is the basis of the modern natural language models. On 2019 OpenAI released GPT-2 which is a 1.5 billion parameters model. This model was trained on 8 million web pages.</p> <p>One year later GPT-3 was released which is a 175 billion parameters model. This model was only accessible through an API due to its enormous size.</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#2022-dalle-2-and-stable-diffusion","title":"2022 Dalle-2 and Stable diffusion","text":"<p>2022 was the year of the big breakthroughs in the field of generative models. Dalle-2 was released by OpenAI and Stable diffusion was open sourced months later by Stability AI. These models can generate high quality image from text descriptions. Thus they combine language and vision.</p> <p></p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#trends-in-deep-learning","title":"Trends in Deep Learning","text":"<p>Datasets and models are growing exponentially.</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#trends-in-datasets","title":"Trends in datasets","text":"<p>Epoch AI Trends in Training Dataset Sizes</p> <p></p> <p>The plots show an exponential growth in the size of the datasets used to train deep learning models.</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#trends-in-model-size","title":"Trends in model size","text":"<p>Epoch AI Trends in Model Sizes</p> <p></p> <p>The model size of notable Machine Learning systems has grown ten times faster than before since 2018. After 2020 growth has not been entirely continuous: there was a jump of one order of magnitude which persists until today. This is relevant for forecasting model size and thus AI capabilities.</p> <p>We can see that the model size also grows exponentially.</p>"},{"location":"artificial-intelligence/09-deep-learning-evolution/#the-future-of-deep-learning","title":"The future of Deep Learning","text":"<p>Clearly there is a trend towards bigger and bigger models and datasets. However at the same time this year 2022 we have seen that a model of just 2 GB can generate high quality images from text.</p> <p>The combination of vision and language has enabled unprecedented levels of generalization. Although the generalization problem is not solved yet this recipe of using a big multimodal text dataset seems to be a promising path to follow.</p>"},{"location":"brain/01-high-level-functions-brain/","title":"High level functions of the brain","text":"<p>To be able to understand and recreate intelligence it is important to know which are the functions done by the brain.</p>"},{"location":"brain/01-high-level-functions-brain/#perception","title":"Perception","text":"<p>Perception is the organization, identification, and interpretation of sensory information in order to represent and understand the presented information or environment.1</p> <p>Perception might be the brain function that it is currently best done by artificial intelligence. There are many models that can predict which objects are present on a image.</p>"},{"location":"brain/01-high-level-functions-brain/#language","title":"Language","text":"<p>A language is a structured system of communication. The structure of a language is its grammar and the free components are its vocabulary. Languages are the primary means of communication of humans, and can be conveyed through speech (spoken language), sign, or writing. 2</p> <p>The Transformer architecture has recently allowed to create very powerful language models like GPT3 and Palm.</p>"},{"location":"brain/01-high-level-functions-brain/#memory","title":"Memory","text":"<p>Memory is the faculty of the mind by which data or information is encoded, stored, and retrieved when needed. It is the retention of information over time for the purpose of influencing future action. If past events could not be remembered, it would be impossible for language, relationships, or personal identity to develop. 3</p> <p>Language models like GPT3 have memorized part of the data they were trained on. However that is very different from human memory. Recurrent models have also some working memory that can modify their behaviour. I believe there is a big gap in this area between the brain and artificial neural networks.</p>"},{"location":"brain/01-high-level-functions-brain/#short-term-memory","title":"Short-term memory","text":"<p>TODO</p>"},{"location":"brain/01-high-level-functions-brain/#long-term-memory","title":"Long-term memory","text":"<p>TODO</p>"},{"location":"brain/01-high-level-functions-brain/#planning","title":"Planning","text":"<p>Planning is the process of thinking regarding the activities required to achieve a desired goal. Planning is based on foresight, the fundamental capacity for mental time travel. The evolution of forethought, the capacity to think ahead, is considered to have been a prime mover in human evolution. Planning is a fundamental property of intelligent behavior. It involves the use of logic and imagination to visualise not only a desired end result, but the steps necessary to achieve that result. 4</p> <p>To be able to plan a model of the world is needed. The human brain is always making predictions about what are we going to experience and if something is different to the prediction attention is risen. This constant predictions are done unconsciously and continuously.</p> <p>I have the believe that this mechanism of prediction of future events could be a very powerful signal to learn useful features for perception.</p>"},{"location":"brain/01-high-level-functions-brain/#attention","title":"Attention","text":"<p>Attention is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether considered subjective or objective, while ignoring other perceivable information. Attention has also been described as the allocation of limited cognitive processing resources. Attention is manifested by an attentional bottleneck, in term of the amount of data the brain can process each second; for example, in human vision, only less than 1% of the visual input data (at around one megabyte per second) can enter the bottleneck,[3][4] leading to inattentional blindness. 5</p> <p>We are receiving a lot of information and it seems that we cannot attend to all of it consciously. Instead we have to focus on a small part of the inputs. In the previous planning section we have advanced that surprising events can gather the attention.</p>"},{"location":"brain/01-high-level-functions-brain/#judgment","title":"Judgment","text":""},{"location":"brain/01-high-level-functions-brain/#decision-making","title":"Decision-making","text":"<p>In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action. 6</p> <p>It is interesting because I was thinking that decision-making was the same as planning, but the irrational part changes all. That links the process of decision-making to the emotions.</p>"},{"location":"brain/01-high-level-functions-brain/#emotion","title":"Emotion","text":"<p>Emotions are mental states brought on by neurophysiological changes, variously associated with thoughts, feelings, behavioural responses, and a degree of pleasure or displeasure. There is currently no scientific consensus on a definition. Emotions are often intertwined with mood, temperament, personality, disposition, or creativity. 7</p>"},{"location":"brain/01-high-level-functions-brain/#motor-skills","title":"Motor skills","text":""},{"location":"brain/01-high-level-functions-brain/#references","title":"References","text":"<ul> <li>Higher Brain Functions</li> <li>Brain Anatomy and How the Brain Works</li> <li>Functions of the Brain</li> </ul> <ol> <li> <p>https://en.wikipedia.org/wiki/Perception\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Language\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Memory\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Planning\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Attention\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Decision-making\u00a0\u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Emotion\u00a0\u21a9</p> </li> </ol>"},{"location":"brain/02-sleep/","title":"Why do we have to sleep","text":"<p>Sleep, Learning, and Memory</p> <p>Research suggests that sleep helps learning and memory in two distinct ways:</p> <ol> <li>A sleep-deprived person cannot focus attention optimally and therefore cannot learn efficiently.</li> <li>Sleep itself has a role in the consolidation of memory, which is essential for learning new information.</li> </ol> <p>Although the exact mechanisms are not known, learning and memory are often described in terms of three functions:</p> <ul> <li>Acquisition refers to the introduction of new information into the brain.</li> <li>Consolidation represents the processes by which a memory becomes stable.</li> <li>Recall refers to the ability to access the information (whether consciously or unconsciously) after it has been stored.</li> </ul> <p>Each of these steps is necessary for proper memory function. Acquisition and recall occur only during wakefulness, but research suggests that memory consolidation takes place during sleep through the strengthening of the neural connections that form our memories.</p> <p>When we sleep we are almost disconnected from the external world, we do not process the information of our senses. It seems that this time is used by the brain to consolidate new memories. It may look that the brain is finding an equilibrium to maintain old memories while adding the new ones (to avoid catastrophical forgetting).</p> <p>I find interesting that the younger the person the more sleep it needs. This is specially obvious in babies where they have to sleep many times a day.</p>"},{"location":"brain/03-data-efficiency/","title":"Data efficiency","text":"<p>Let's see how much data current deep learnings systems need compared to humans:</p> <ul> <li>Llama 2 is trained on 2e12 tokens. On OpenAI's documentation it is said that 128k tokens are equivalent to 240 pages (at 400 words per page). Let's say that the average book has 240 pages. So 2e12 tokens is equivalent to 15 million books. A human will need 42000 years to read those books if reading one book a day.</li> <li>Laion-5B is a dataset with 5.8e9 image-text pairs that is used to train CLIP models. If a human spend all the day watching the images at a rate of one per second it would need 184 years (without sleeping).</li> <li>On Alphastar they train each agent for the equivalent of 200 human years of of real-time StarCraft play.</li> <li>AlphaZero played 44 million chess games, if each chess game takes around 20 minutes that would be 33 years of play.</li> <li>Minecraft from Video Pretraining the model is trained to imitate humans for 70k hours, which is 8 years. This is different than learning how to play by its own.</li> </ul> dataset equivalent human years Llama 2 42000 Laion 5B 184 AlphaStar 200 AlphaZero chess 33 Minecraft 8 <p>Neither Llama 2 or CLIP models have matched the capabilities of humans, despite using much more data. Why?</p> <ul> <li>Priors. The human brain is likely using good priors over the real world data that reduce the search space and allow to learn from less data. Evolution would very likely favour this kind of priors. We might be missing some priors in our models, or using wrong ones.</li> <li>Data quality. Sometimes we have very large datasets but low diversity or quality, so the effective size is much smaller than the number suggest.</li> <li>Data distribution. When humans see an object for a first time they are able to see the object from varius distances and angles. This data distribution is very different from Laion where each image is typically independent from the others.</li> <li>Interaction of the world. As humans we have constant interaction with the world. That allows to learn a very accurate world model that will be corrected anytime it makes a wrong prediction. In the other hand current deep learning systems typically just receive passive data and are simply required to copy what humans do, without the oportunity to interact with the world.</li> </ul> <p>A 5 year old human does have a much better vision system than CLIP and a much more general intelligence than Llama 2. It seems we are missing something very relevant in our current models.</p>"},{"location":"business/01-advertising/","title":"Advertising","text":"<p>Let't get some basic numbers to understand how we could monetize machine learning models using advertising.</p>"},{"location":"business/01-advertising/#typical-costs","title":"Typical costs","text":""},{"location":"business/01-advertising/#cost-per-mille-cpm","title":"Cost per mille (CPM)","text":"<p>Cost per mille, also called cost per thousand, is a commonly-used measurement in advertising. It is the cost an advertiser pays for one thousand views or impressions of an advertisement.</p> <p>https://dashthis.com/kpi-examples/cost-per-thousand/</p> <p>the average online advertising cost per thousand impressions an advertiser pays would be around \\$3-\\$10</p> <p>ChatGPT also says that it is in the range of \\$1-10. This applies both to a website and to a Youtube video.</p>"},{"location":"business/01-advertising/#cost-per-click-cpc","title":"Cost per click (CPC)","text":"<p>https://www.webfx.com/ppc/glossary/what-is-cpc/</p> <p>The average cost for a Google Ads CPC campaign is \\$1 to \\$2. If you\u2019re advertising on the Google Display Network, the average CPC is less than \\$1.</p> <p>This has sense, the cost per click is lower than the cost per mille because not all the people that see the ad will click on it. In fact it implies that the click through rate (CTR) is around than 0.1%.</p>"},{"location":"business/01-advertising/#advertising-on-smartphones","title":"Advertising on Smartphones","text":"<p>https://www.myhoardings.com/ads/how-much-does-it-cost-to-advertise-on-a-mobile-app/</p> <ol> <li>The average U.S. rewarded videos\u2019 CPMs is around \\$15 for iOS and \\$11 for Android</li> <li>The average CPM for banner all Android devices is \\$0,4 and \\$0.5 for iOS</li> <li>The average CPM for full-screen ads \\$ 9.50 for IOS and around 6\\$ for Android.</li> </ol>"},{"location":"business/01-advertising/#summary","title":"Summary","text":"monetization cost ($) Cost per mille 3-10 Cost per click 1-2 Rewarded video 11-15 Smartphone banner 0.4-0.5 Full-screen add 6-9.5"},{"location":"business/01-advertising/#sample-cases","title":"Sample cases","text":""},{"location":"business/01-advertising/#self-hosted-model","title":"Self-hosted model","text":"<p>Let's imagine we are running an API on a Nvidia T4 GPU. The prize on HuggingFace is \\$0.60 per hour. Let's consider the worst scenario for each of the previous types of monetization. (I have removed the cost per click because it should be equivalent to the cost per mille).</p> monetization min revenue ($) hourly min visitors monthly min visitors max inference time (s) Smartphone banner 0.4 1500 1.1E+06 2.4 Cost per mille 3 200 1.4E+05 18 Rewarded video 11 55 3.9E+04 66 Full-screen add 6 100 7.2E+04 36 <p>So to have a profitable business we will need more than 1M visitors if using an app with banners. In the other hand using rewarded videos we will need less than 40K visitors. One interesting option would be to display the video when the image is being generated.</p> <p>This monetization could be combined with a subscription model to remove the ads.</p>"},{"location":"business/01-advertising/#calling-to-an-api","title":"Calling to an API","text":"monetization min revenue ($) max API cost ($) Smartphone banner 0.4 0.0004 Cost per mille 3 0.003 Rewarded video 11 0.011 Full-screen add 6 0.006 <p>GPT4-Vision call is around 0.01\\$, thus only a rewarded video could pay for that.</p>"},{"location":"business/01-advertising/#summary_1","title":"Summary","text":"<p>It is possible to monetize a machine learning model using ads.</p>"},{"location":"projects/01-connect-4/","title":"Connect 4","text":""},{"location":"projects/01-connect-4/#goals","title":"Goals","text":"<ol> <li>Create an AI that is able to beat me on Connect 4 game</li> <li>Learn from the AI what is the best policy</li> <li>Compare model-free and model-based algorithms on this game</li> </ol>"},{"location":"projects/01-connect-4/#motivation","title":"Motivation","text":"<p>When I play Connect 4 I tipically place my first pieces in the center. I don't know if that is the best policy and I would like to know it. I believe it will be fun to create a superhuman AI for this game and hopefully I will learn something.</p>"},{"location":"projects/01-connect-4/#about-the-game","title":"About the game","text":"<p>Let's first read about the game to see if it solved. From the wikipedia</p> <p>Connect Four is a two-player connection board game, in which the players choose a color and then take turns dropping colored tokens into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own tokens. Connect Four is a solved game. The first player can always win by playing the right moves.</p> <p>So we can see that the game has been solved.</p> <p>For classic Connect Four played on a 7-column-wide, 6-row-high grid, there are 4,531,985,219,092 positions (4.5e12) for all game boards populated with 0 to 42 pieces.</p> <p>The state space is very big, so we cannot store a value table.</p> <p>The solved conclusion for Connect Four is first-player-win. With perfect play, the first player can force a win, on or before the 41st move by starting in the middle column.</p> <p>Let's see if we can get an agent close to the optimum.</p>"},{"location":"projects/01-connect-4/#environment","title":"Environment","text":"<p>We need to find an environment where we can train the agents and also play against them.</p>"},{"location":"projects/01-connect-4/#the-job-was-already-done","title":"The job was already done","text":"<p>I have found that Anthony Young has already trained an alpha zero agent on connect 4. It has even a website where it is possible to play against the agent, although I get connection errors very often.</p> <p>There are a lot of implementation details so it is worth reading the blog. It says that connect 4 is a good testbed for verifying that alpha zero implementation works correctly. So if in the future I have to do it I could use it.</p> <p>The version deployed on the website does not do search but instead uses just the learned policy.</p>"},{"location":"projects/01-connect-4/#game-dynamics","title":"Game dynamics","text":"<p>My intuition was correct, the center column is filled to the top at first. The predictions of the different generations of agents do not seem to be stable.</p>"},{"location":"projects/01-connect-4/#thoughts-about-multiplayer-games","title":"Thoughts about multiplayer games","text":"<p>Model free methods tipically learn a value function. In a multiplayer game the skill of the opponent will influence that value function. If we play against different opponents during training we will be adding noise to the value function and thus making the learning more difficult.</p> <p>Always playing against the same player will result in a more stable training. But there is the risk of overfitting to that player by exploiting the weak points of the player.</p> <p>A policy could learn to play very well, but planning is more general because it will work well when finding new situations that were not found during training.</p>"},{"location":"thought-experiments/01-house-robot/","title":"House robot","text":"<p>Let's imagine we are already in the future and we have a home robot</p> <p></p>"},{"location":"thought-experiments/01-house-robot/#case-1","title":"Case 1","text":"<pre><code>Human: Clean the bathroom\nRobot: I don't know how to clean the bathroom\n</code></pre> <p>To be able to reach this situation the robot needs to query its world model something like: Do I know how to clean the bathroom? Have I ever clean the bathroom?</p> <p>Or maybe the robot queries its world model: What is the first thing I have to do to clean the bathroom? And the answer will be: I don't know.</p>"},{"location":"thought-experiments/01-house-robot/#world-model","title":"World model","text":"<p>So the robot needs a world model to be able to do the tasks. What is the difference between a world model and a huge database of data, f.e. all the memories of the robot? Probably the difference is that I can query the world model and get an answer. On a database I can query it and get data in return but data itself is not an answer. Also the world model can use its knowledge to answer about events that have never happened and imagine the future. Also the world model needs to handle uncertainty and lack of knowledge.</p> <p>For this task a database could be enough. The robot could have a library with different routines and it queries for the cleaning bathroom routing and since it cannot find it, it is aware that it does not know how to do it.</p>"},{"location":"thought-experiments/01-house-robot/#case-2","title":"Case 2","text":"<pre><code>Human: Clean the bathroom\nRobot: Ok Master\n</code></pre> <p>After answering the robot does the following actions:</p> <ol> <li>Goes to the closet and picks up the broom</li> <li>Goes to bathroom and starts cleaning</li> <li>Evaluates the cleanliness of the floor and decides to stop cleaning</li> <li>Returns the broom to the closet</li> </ol> <p>To be able to do this tasks the robot needs to have an algorithm to do the cleaning. It needs to know the different steps of the task and the success criteria of each step. It also needs a policy for each step and a critic that decides if the success criteria is met.</p> <p>Can the world model do both?</p>"},{"location":"thought-experiments/01-house-robot/#case-3","title":"Case 3","text":"<pre><code>Human: Clean the bathroom\nRobot: Which bathroom master?\n</code></pre> <p>In this case the robot knows how to clean the bathroom but since there are two bathrooms in the house there is uncertainty regarding which one needs to be cleaned.</p>"}]}